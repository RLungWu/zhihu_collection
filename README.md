# zhihu_collection

| No.| Title | Summary | Feedback |
| :--: | :--: | :--: |:--:| 
|1| [LLM 推理加速](https://zhuanlan.zhihu.com/p/17811924624?utm_psn=1861592084353540096)  |  |  |
|2|[大模型的基本功](https://zhuanlan.zhihu.com/p/716344766?utm_psn=1857956031679840257)|
|3|[DPO对齐，用脏话数据教AI说真话](https://zhuanlan.zhihu.com/p/18945731214)|
|4|[手搓一个最小的 Agent 系统 — Tiny Agent](https://zhuanlan.zhihu.com/p/699732624)|
|5|[一文讲明白大模型显存占用（只考虑单卡）](https://zhuanlan.zhihu.com/p/713256008?utm_psn=1850877468791164928)|
|6|[Nips2024-Agent方向论文（上）](https://zhuanlan.zhihu.com/p/12317898153?utm_psn=1851404730472992768)|
|7|[基于LLM+向量库的文档对话痛点及解决方案](https://zhuanlan.zhihu.com/p/651179780?utm_psn=1851983465051975682)|
|8|[RAG（检索增强生成）会不会消亡呢？](https://www.zhihu.com/question/637421964/answer/33309712475?utm_psn=1852132878818869248)|
|9|[LightRAG技术框架解读](https://zhuanlan.zhihu.com/p/13261291813?utm_psn=1853075329633091584)|
|10|[大家觉得做一个大模型检索增强生成（RAG）系统，最难搞定的是那部分工作？](https://www.zhihu.com/question/642650878/answer/57968449225?utm_psn=1853197485889163265)|
|11|[万字长文梳理2024年的RAG](https://zhuanlan.zhihu.com/p/14116449727?utm_psn=1854663568068308992)|
|12|[我没有大模型经验，可以给个机会吗？](https://zhuanlan.zhihu.com/p/715031517?utm_psn=1857139809681805312)|
|13|[大模型如何高效部署？CMU最新万字综述纵览LLM推理MLSys优化技术](https://zhuanlan.zhihu.com/p/677635306?utm_psn=1857485262482989056)|
|14|[大模型的基本功](https://zhuanlan.zhihu.com/p/716344766?utm_psn=1857956031679840257)|
|15|[2024年大模型基础设施领域（训练、推理、硬件）有什么值得关注研究方向？](https://www.zhihu.com/question/637480772/answer/3474883101?utm_psn=1857961419003269120)|
|16|[LLM 推理加速](https://zhuanlan.zhihu.com/p/17811924624?utm_psn=1861592084353540096)|
|17|[为什么4090速度比A100快很多呢？](https://www.zhihu.com/question/615946801/answer/3205148871?utm_psn=1862858783665049600)|
|18|[LLM 推理加速方式汇总](https://zhuanlan.zhihu.com/p/688736901?utm_psn=1863541831435968512)|
|19|[mini_qwen：从0开始训练1B参数的大模型](https://zhuanlan.zhihu.com/p/19353252686?utm_psn=1864662479939985408)|
|20|[【代码+数据集】DPO对齐，用脏话数据教AI说真话](https://zhuanlan.zhihu.com/p/18945731214?utm_psn=1865208193703079937)|
|21|[SFT、RLHF、DPO、IFT —— LLM 微调的进化之路](https://zhuanlan.zhihu.com/p/710652762?utm_psn=1865532802738552833)|
|22|[如何评价 DeepSeek 的 DeepSeek-V3 模型？](https://www.zhihu.com/question/7837132971/answer/72079893262?utm_psn=1866102231436230656)|
|23|[给AI喂了4000句脏话，它竟然开始觉醒说真话了！(附代码)](https://zhuanlan.zhihu.com/p/18745659547?utm_psn=1868373722618028032)|
|24|[手搓一个最小的 Agent 系统 — Tiny Agent](https://zhuanlan.zhihu.com/p/699732624?utm_psn=1869109101595873280)|
|25|[大模型炼丹术：大模型微调总结及实现](https://zhuanlan.zhihu.com/p/673789772?utm_psn=1869111520702648321)|
|26|[Deepseek V3 预训练解读](https://zhuanlan.zhihu.com/p/15073492309?utm_psn=1869173633865375746)|
|27|[深入理解 Megatron-LM（1）基础知识](https://zhuanlan.zhihu.com/p/650234985?utm_psn=1869377491669479425)|
|28|[大家觉得做一个大模型检索增强生成（RAG）系统，最难搞定的是那部分工作？](https://www.zhihu.com/question/642650878/answer/86323321960?utm_psn=1871330803125997569)|
|29|[LLM的MoE架构的“动态路由”为什么能训练出来？](https://www.zhihu.com/question/11450572647/answer/95847905917?utm_psn=1871984789910872064)|
|30|[再读MLA，还有多少细节是你不知道的](https://zhuanlan.zhihu.com/p/19585986234?utm_psn=1871987158165897216)|
|31|[【必看】LLM历史技术文章导航](https://zhuanlan.zhihu.com/p/654910335?utm_psn=1871995513445957632)|
|32|[大模型推理加速技术的学习路线是什么?](https://www.zhihu.com/question/591646269/answer/3333654552?utm_psn=1872750354279559168)|
|33|[deepseek技术解读(1)-彻底理解MLA（Multi-Head Latent Attention）](https://zhuanlan.zhihu.com/p/16730036197?utm_psn=1872639045261217792)|
|34|[拆解大语言模型RLHF中的PPO](https://zhuanlan.zhihu.com/p/645225982?utm_psn=1872250276049715201)|
|35|[开源DeepResearch：24小时挑战，重现OpenAI的AI Agent奇迹](https://zhuanlan.zhihu.com/p/22559225933?utm_psn=1872248791379357697)|
|36|[一个博士生接受怎样的训练是完整、全面的科研训练？](https://www.zhihu.com/question/384512106/answer/3556664044)|
