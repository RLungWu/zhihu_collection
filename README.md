# zhihu_collection

| No.| Title | Summary | Feedback |
| :--: | :--: | :--: |:--:| 
|1| [LLM 推理加速](https://zhuanlan.zhihu.com/p/17811924624?utm_psn=1861592084353540096)  |  |  |
|2|[大模型的基本功](https://zhuanlan.zhihu.com/p/716344766?utm_psn=1857956031679840257)|
|3|[DPO對齊，用臟話數據教AI說真話](https://zhuanlan.zhihu.com/p/18945731214)|
|4|[手搓一個最小的 Agent 系統 — Tiny Agent](https://zhuanlan.zhihu.com/p/699732624)|
|5|[一文講明白大模型顯存占用（只考慮單卡）](https://zhuanlan.zhihu.com/p/713256008?utm_psn=1850877468791164928)|
|6|[Nips2024-Agent方向論文（上）](https://zhuanlan.zhihu.com/p/12317898153?utm_psn=1851404730472992768)|
|7|[基於LLM+向量庫的文檔對話痛點及解決方案](https://zhuanlan.zhihu.com/p/651179780?utm_psn=1851983465051975682)|
|8|[RAG（檢索增強生成）會不會消亡呢？](https://www.zhihu.com/question/637421964/answer/33309712475?utm_psn=1852132878818869248)|[summary](./zhihu/08/readme.md)|
|9|[LightRAG技術框架解讀](https://zhuanlan.zhihu.com/p/13261291813?utm_psn=1853075329633091584)|
|10|[大家覺得做一個大模型檢索增強生成（RAG）系統，最難搞定的是那部分工作？](https://www.zhihu.com/question/642650878/answer/57968449225?utm_psn=1853197485889163265)|
|11|[萬字長文梳理2024年的RAG](https://zhuanlan.zhihu.com/p/14116449727?utm_psn=1854663568068308992)|
|12|[我沒有大模型經驗，可以給個機會嗎？](https://zhuanlan.zhihu.com/p/715031517?utm_psn=1857139809681805312)|
|13|[大模型如何高效部署？CMU最新萬字綜述縱覽LLM推理MLSys優化技術](https://zhuanlan.zhihu.com/p/677635306?utm_psn=1857485262482989056)|
|14|[大模型的基本功](https://zhuanlan.zhihu.com/p/716344766?utm_psn=1857956031679840257)|
|15|[2024年大模型基礎設施領域（訓練、推理、硬件）有什麽值得關注研究方向？](https://www.zhihu.com/question/637480772/answer/3474883101?utm_psn=1857961419003269120)|
|16|[LLM 推理加速](https://zhuanlan.zhihu.com/p/17811924624?utm_psn=1861592084353540096)|
|17|[為什麽4090速度比A100快很多呢？](https://www.zhihu.com/question/615946801/answer/3205148871?utm_psn=1862858783665049600)|
|18|[LLM 推理加速方式匯總](https://zhuanlan.zhihu.com/p/688736901?utm_psn=1863541831435968512)|
|19|[mini_qwen：從0開始訓練1B參數的大模型](https://zhuanlan.zhihu.com/p/19353252686?utm_psn=1864662479939985408)|
|20|[【代碼+數據集】DPO對齊，用臟話數據教AI說真話](https://zhuanlan.zhihu.com/p/18945731214?utm_psn=1865208193703079937)|
|21|[SFT、RLHF、DPO、IFT —— LLM 微調的進化之路](https://zhuanlan.zhihu.com/p/710652762?utm_psn=1865532802738552833)|
|22|[如何評價 DeepSeek 的 DeepSeek-V3 模型？](https://www.zhihu.com/question/7837132971/answer/72079893262?utm_psn=1866102231436230656)|
|23|[給AI喂了4000句臟話，它竟然開始覺醒說真話了！(附代碼)](https://zhuanlan.zhihu.com/p/18745659547?utm_psn=1868373722618028032)|
|24|[手搓一個最小的 Agent 系統 — Tiny Agent](https://zhuanlan.zhihu.com/p/699732624?utm_psn=1869109101595873280)|
|25|[大模型煉丹術：大模型微調總結及實現](https://zhuanlan.zhihu.com/p/673789772?utm_psn=1869111520702648321)|
|26|[Deepseek V3 預訓練解讀](https://zhuanlan.zhihu.com/p/15073492309?utm_psn=1869173633865375746)|
|27|[深入理解 Megatron-LM（1）基礎知識](https://zhuanlan.zhihu.com/p/650234985?utm_psn=1869377491669479425)|
|28|[大家覺得做一個大模型檢索增強生成（RAG）系統，最難搞定的是那部分工作？](https://www.zhihu.com/question/642650878/answer/86323321960?utm_psn=1871330803125997569)|
|29|[LLM的MoE架構的“動態路由”為什麽能訓練出來？](https://www.zhihu.com/question/11450572647/answer/95847905917?utm_psn=1871984789910872064)|
|30|[再讀MLA，還有多少細節是你不知道的](https://zhuanlan.zhihu.com/p/19585986234?utm_psn=1871987158165897216)|
|31|[【必看】LLM歷史技術文章導航](https://zhuanlan.zhihu.com/p/654910335?utm_psn=1871995513445957632)|
|32|[大模型推理加速技術的學習路線是什麽?](https://www.zhihu.com/question/591646269/answer/3333654552?utm_psn=1872750354279559168)|
|33|[deepseek技術解讀(1)-徹底理解MLA（Multi-Head Latent Attention）](https://zhuanlan.zhihu.com/p/16730036197?utm_psn=1872639045261217792)|
|34|[拆解大語言模型RLHF中的PPO](https://zhuanlan.zhihu.com/p/645225982?utm_psn=1872250276049715201)|
|35|[開源DeepResearch：24小時挑戰，重現OpenAI的AI Agent奇跡](https://zhuanlan.zhihu.com/p/22559225933?utm_psn=1872248791379357697)|Duplicate|感覺無用|
|36|[一個博士生接受怎樣的訓練是完整、全面的科研訓練？](https://www.zhihu.com/question/384512106/answer/3556664044)|[Summary](./zhihu/36/No36.md)||
|37|[RAG（檢索增強生成）會不會消亡呢？](https://www.zhihu.com/question/637421964/answer/99446183305?utm_psn=1873430932557537280)|
|38|[從零到一打造商用 AI Agent（智能體)](https://zhuanlan.zhihu.com/p/23285829505?utm_psn=1873432769390051328)|
|39|[大模型推理加速技術的學習路線是什麽?](https://www.zhihu.com/question/591646269/answer/102260978569?utm_psn=1874459723065081858)|
|40|[2025年大模型LLM還有哪些可研究的方向？](https://www.zhihu.com/question/11285951981)|[Summary](./zhihu/41/)|
|41|[解讀Open-source DeepResearch](https://zhuanlan.zhihu.com/p/22733177444)|
|42|[從零實現一個MOE（專家混合模型）](https://zhuanlan.zhihu.com/p/701777558?utm_psn=1875507800106467329)|
|43|[RAG、LangChain、Agent 到底有啥關系？](https://www.zhihu.com/question/2495164206/answer/86964072894?utm_psn=1875623080514158592)|
|44|[7 MLOPs Projects for Begineers](https://www.kdnuggets.com/7-mlops-projects-beginners)|
|45|[如何把deepseek-R1微調/蒸餾為某領域的一個專家？](https://www.zhihu.com/question/10555876430/answer/104851046659)|
|46|[Llama模仿Diffusion多模態漲分30%！只需共享注意力分布](https://zhuanlan.zhihu.com/p/24207646579)|
|47|[從零搭建自己的 Multi-Agent](https://zhuanlan.zhihu.com/p/23104679193?utm_psn=1875954926837964800)||有點限制|
|48|[llm reasoning/ theory of llm 從入門到入土](https://zhuanlan.zhihu.com/p/24665806059?utm_psn=1875959409554903040)|
|49|[筆記：MoBA 與 Native Sparse Attention](https://zhuanlan.zhihu.com/p/24774848974?utm_psn=1875959707941867520)|
|50|[目前大模型量化方案有很多，有哪些比較SOTA的量化方案？](https://www.zhihu.com/question/10439431486/answer/86095724572?utm_psn=1875983114599268352)|
|51|[筆記：關於 LLM MLSys 研究的一些思考](https://zhuanlan.zhihu.com/p/720634180?utm_psn=1877109033598582784)|
|52|[AI Agent目前應用落地有哪些局限性？](https://www.zhihu.com/question/624354739/answer/95596641305?utm_psn=1876811850999533568)|
|53|[筆記：關於 LLM MLSys 研究的一些思考](https://zhuanlan.zhihu.com/p/720634180?utm_psn=1877109033598582784)|
|54|[大模型面試：RAG項目的拷問（文字版）](https://zhuanlan.zhihu.com/p/3448487946?utm_psn=1880380835355669111)|
|55|[以RLer視角看大模型訓練中的強化學習](https://zhuanlan.zhihu.com/p/23290969372?utm_psn=1880357438890415094)|
|56|[從啥也不會到CUDA GEMM優化](https://zhuanlan.zhihu.com/p/703256080?utm_psn=1881410562489034686)|
|57|[學習檢索增強生成 RAG時，多數人是不是從Llamaindex學起，首先研究和改寫哪個RAG開源代碼？](https://www.zhihu.com/question/659305847/answer/119512863129?utm_psn=1881759616964203347)|
|58|[大模型Post-training的範氏可能已經發生改變](https://zhuanlan.zhihu.com/p/28584030587?utm_psn=1881768454979297519)|
|59|[深入理解構建和優化大語言模型推理的方式和策略](https://zhuanlan.zhihu.com/p/28530752474?utm_psn=1881823435275080134)|
|60|[圖解大模型RLHF系列之: 人人都能看懂的PPO原理與原馬解讀](https://zhuanlan.zhihu.com/p/677607581?utm_psn=1882502125289972638)|
|61|[代碼模型碎碎念](https://zhuanlan.zhihu.com/p/29211684588)|
|62|[為什麼QWEN能自我改進推理，LLAMA卻不行? 斯坦福找到了原理。](https://zhuanlan.zhihu.com/p/28978075877)|
|63|[看圖學大模型：Transformers 的前生今世(上）](https://zhuanlan.zhihu.com/p/690984212?utm_psn=1883577238492001675)|
|64|[全景解讀 LLM 後訓練技術](https://zhuanlan.zhihu.com/p/30201040247)|
|65|[AIRI，LoRA 知識集成 在 LoRA 中可裝入多少知識而不損害 LLM？](https://zhuanlan.zhihu.com/p/28312749989?share_code=y63OYF9aHf3W&utm_psn=1887926026383892957)|
|66|[大模型微調到底有沒有技術含量，或者說技術含量到底有多大？](https://www.zhihu.com/question/599396505/answer/1887826439019148076)|
|67|[【論文解讀】DAPO：工業級LLM + RL訓練解決方案](https://zhuanlan.zhihu.com/p/32368626065)|
|68|[吳恩達來信：如何決定是否該進行微調](https://zhuanlan.zhihu.com/p/1888902823573426902?share_code=5t5SbQzT5LMT&utm_psn=1889079600287559690)|
|69|[北美ML求職篇： 業界LLM Project](https://zhuanlan.zhihu.com/p/1888436346835686934?share_code=FsxCt52QCzj3&utm_psn=1889103751429747724)|
|70|[SFT loss 計算的那些坑（多輪合並/packing）](https://zhuanlan.zhihu.com/p/721652210)|
|71|[在 AI Agent 的開發中，交互設計（殼）和底層模型能力哪個更重要？未來哪一方會成為競爭核心？](https://www.zhihu.com/question/1891113677635162799/answer/1891169292898246840?share_code=KfwwgRKt6K4W&utm_psn=1894891833739425647)|
|72|[從啥也不會到CUDA GEMM優化](https://zhuanlan.zhihu.com/p/703256080?utm_psn=1881410562489034686)|
|73|[論文筆記：DejaVu、LLM in Flash、PowerInfer](https://zhuanlan.zhihu.com/p/675585887)|
|74|[聊聊大模型推理的動態稀疏化之二：MInference1 與 PowerInfer2](https://zhuanlan.zhihu.com/p/711399792)|
|75|[有沒有一本講解gpu和CUDA編程的經典入門書籍](https://www.zhihu.com/question/26570985/answer/3465784970)|
|76|[CUDA學習資料及路線圖【持續更新】](https://zhuanlan.zhihu.com/p/273607744)|
|77|[RAG：LLMs + LangChain app 開發關鍵技術整理](https://zhuanlan.zhihu.com/p/664097125)|
|78|[如何評價 Deepseek 新發布的 DeepSeek-Prover-V2-671B 模型?](https://www.zhihu.com/question/1900955534003254030)|
|79|[Llama-index如何實現大模型RAG？有哪些方式和途徑？](https://www.zhihu.com/question/644321945/answer/3506765564)|
|80|[大模型微調（finetune）方法總結](https://zhuanlan.zhihu.com/p/644122818?share_code=13yG7QnktUqMr&utm_psn=1901439006358639617)|
|81|[RAG：開發流程](https://zhuanlan.zhihu.com/p/1901230247434772975?share_code=MyuzYIDrd0N2&utm_psn=1901822247016040195)|
|82|[本科生如何學習ROS呢?](https://www.zhihu.com/question/591518177/answer/2953919733?share_code=tfjwOcmID6Wd&utm_psn=1907137089625134797)|
|83|[多模態大模型(理解)入門看這一篇就好](https://zhuanlan.zhihu.com/p/706216503?share_code=1irFdBC2pFiPT&utm_psn=1907550253818815726)|
|84|[ISSAC SIM 入門採坑指南](https://zhuanlan.zhihu.com/p/27174960684?share_code=1emWKon8u2B0U&utm_psn=1908184201892701685)|
|85|[大家在工作中都甚麼時候會用到Open ai Triton](https://www.zhihu.com/question/635910409/answer/3340587355?share_code=CIbgQndNVrye&utm_psn=1902675302477435139)|
|86|[大模型所有算子邏輯](https://zhuanlan.zhihu.com/p/1909996866432668841?share_code=ijdBWGdx6vPY&utm_psn=1910678952382567991)|
|87|[博三上NLP方向，有2卡A100，能夠微調llama，chatglm等模型，找不著研究方向怎麽辦？](https://www.zhihu.com/question/629933996/answer/6648927215?share_code=z1rIEk6pvfMo&utm_psn=1913936790009673254)|
|88|[Qwen團隊新發現：大模型推理能力的提高僅由少數高熵 Token 貢獻](https://zhuanlan.zhihu.com/p/1913555493412115868?share_code=17ZGbyN6HekmE&utm_psn=1914251462931902550)|
|89|[無人機應用級開發（建圖/視覺/強化學習）應該如何入門？如何搭一個可以實現算法的無人機實驗平台？](https://www.zhihu.com/question/336001585/answer/756146695)|
|90|[2025 AI模型選用指南](https://zhuanlan.zhihu.com/p/1914348033820328694?share_code=EIuu75dQWdu9&utm_psn=1914381702484197858)|
|91|[為什麽深度學習pytorch庫里的torch.nn的一些函數可以實現的東西要用類實現，比如卷積層？](https://www.zhihu.com/question/677187311/answer/3780895706?share_code=jsM9M3SCpxRZ&utm_psn=1915043615350192040)|
|92|[最新綜述！具身領域中基於物理模擬器的機器人導航與操作](https://zhuanlan.zhihu.com/p/1905403054779524293)|
|93|[VLN 視覺導航入門](https://zhuanlan.zhihu.com/p/19435123237)|
|94|[大模型LLM-微調經驗分享&總結](https://zhuanlan.zhihu.com/p/620885226)|[Summary](./zhihu/94/readme.md)|
|94|[SFT - 22 條經驗](https://zhuanlan.zhihu.com/p/49398269658)|
|95|[CVPR 2025 有哪些值得關注的文章?](https://www.zhihu.com/question/13320524361)|
|96|[LLM4Rec-Day15 - RAG](https://zhuanlan.zhihu.com/p/1923410628132999893)|